{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_JclOZro-J1",
        "outputId": "419018a2-83d5-4558-c55c-f18df6219bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "### 데이터 로딩 ###\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/빅데이터 팀플/dementia_analytics/data_processing/dataset_05/dataset_with_label_fill_user_mean.csv')\n",
        "data_output = dataset[['DIAG_NM']]\n",
        "# data_input에서 score 가 들어가는 column 등 필요 없는 column 제거\n",
        "data_input = dataset.drop(['DIAG_NM', 'ID', 'date'], axis=1)\n",
        "columns_to_drop = [col for col in data_input.columns if 'score' in col]\n",
        "data_input = data_input[['sleep_breath_average', 'sleep_hr_average', 'sleep_hr_lowest', 'sleep_deep', 'sleep_rem', 'activity_cal_total', 'sleep_awake', 'activity_steps', 'activity_total', 'sleep_duration', 'activity_daily_movement']]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_input_data, test_input_data, train_output_data, test_output_data = train_test_split(data_input, data_output, test_size=0.2, shuffle=True, stratify=data_output, random_state=42)\n",
        "\n",
        "### 레이블 인코딩 ### \n",
        "\n",
        "# 트리 기반 알고리즘에서는 숫자의 크기에 따른 중요도 차이가 없어 레이블 인코딩 OK\n",
        "# 선형적 특징을 가지는 알고리즘들은 one hot encoding 통해 해결해야 함\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 라벨인코더 선언 및 Fitting\n",
        "# CN : 0, Dem : 1, MCI : 2\n",
        "items = ['CN', 'Dem', 'MCI']\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "\n",
        "# train\n",
        "train_output_data['DIAG_NM'] = encoder.transform(train_output_data['DIAG_NM'])\n",
        "\n",
        "# test\n",
        "test_output_data['DIAG_NM'] = encoder.transform(test_output_data['DIAG_NM'])\n"
      ],
      "metadata": {
        "id": "s-UM5qpXi6hM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_input, val_input, train_output, val_output = train_test_split(train_input_data, train_output_data, test_size=0.2, shuffle=True, stratify=train_output_data, random_state=42)"
      ],
      "metadata": {
        "id": "oZBZKdFli8F8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_input, v_input, t_output, v_output = train_test_split(train_input, train_output, test_size=0.2, shuffle=True, stratify=train_output, random_state=42)"
      ],
      "metadata": {
        "id": "XdZdc65Wi90B"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_fit = scaler.fit(t_input)\n",
        "t_input = scaler_fit.transform(t_input)\n",
        "v_input = scaler_fit.transform(v_input)\n",
        "val_input = scaler_fit.transform(val_input)"
      ],
      "metadata": {
        "id": "FOmZz_Xpi-_o"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "sm = SMOTE(random_state=32)\n",
        "\n",
        "t_input, t_output = sm.fit_resample(t_input, t_output)"
      ],
      "metadata": {
        "id": "xBOYqQo8jCOs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "728FJfUWp7u_"
      },
      "source": [
        "# LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9Fb_agHvqgGF",
        "outputId": "1cf32168-0a1b-4346-ba82-2be0daae9b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\tvalid_0's multi_logloss: 1.05031\n",
            "[2]\tvalid_0's multi_logloss: 1.00938\n",
            "[3]\tvalid_0's multi_logloss: 0.972824\n",
            "[4]\tvalid_0's multi_logloss: 0.941578\n",
            "[5]\tvalid_0's multi_logloss: 0.915929\n",
            "[6]\tvalid_0's multi_logloss: 0.891269\n",
            "[7]\tvalid_0's multi_logloss: 0.871676\n",
            "[8]\tvalid_0's multi_logloss: 0.85351\n",
            "[9]\tvalid_0's multi_logloss: 0.836038\n",
            "[10]\tvalid_0's multi_logloss: 0.819527\n",
            "[11]\tvalid_0's multi_logloss: 0.80263\n",
            "[12]\tvalid_0's multi_logloss: 0.788233\n",
            "[13]\tvalid_0's multi_logloss: 0.775014\n",
            "[14]\tvalid_0's multi_logloss: 0.761681\n",
            "[15]\tvalid_0's multi_logloss: 0.747683\n",
            "[16]\tvalid_0's multi_logloss: 0.735243\n",
            "[17]\tvalid_0's multi_logloss: 0.72487\n",
            "[18]\tvalid_0's multi_logloss: 0.714978\n",
            "[19]\tvalid_0's multi_logloss: 0.705395\n",
            "[20]\tvalid_0's multi_logloss: 0.697004\n",
            "[21]\tvalid_0's multi_logloss: 0.687761\n",
            "[22]\tvalid_0's multi_logloss: 0.680624\n",
            "[23]\tvalid_0's multi_logloss: 0.6724\n",
            "[24]\tvalid_0's multi_logloss: 0.665746\n",
            "[25]\tvalid_0's multi_logloss: 0.658214\n",
            "[26]\tvalid_0's multi_logloss: 0.652283\n",
            "[27]\tvalid_0's multi_logloss: 0.64532\n",
            "[28]\tvalid_0's multi_logloss: 0.640034\n",
            "[29]\tvalid_0's multi_logloss: 0.634302\n",
            "[30]\tvalid_0's multi_logloss: 0.629622\n",
            "[31]\tvalid_0's multi_logloss: 0.624197\n",
            "[32]\tvalid_0's multi_logloss: 0.61854\n",
            "[33]\tvalid_0's multi_logloss: 0.613588\n",
            "[34]\tvalid_0's multi_logloss: 0.609265\n",
            "[35]\tvalid_0's multi_logloss: 0.605378\n",
            "[36]\tvalid_0's multi_logloss: 0.601236\n",
            "[37]\tvalid_0's multi_logloss: 0.597004\n",
            "[38]\tvalid_0's multi_logloss: 0.592286\n",
            "[39]\tvalid_0's multi_logloss: 0.588926\n",
            "[40]\tvalid_0's multi_logloss: 0.586949\n",
            "[41]\tvalid_0's multi_logloss: 0.583444\n",
            "[42]\tvalid_0's multi_logloss: 0.580052\n",
            "[43]\tvalid_0's multi_logloss: 0.576306\n",
            "[44]\tvalid_0's multi_logloss: 0.573659\n",
            "[45]\tvalid_0's multi_logloss: 0.570559\n",
            "[46]\tvalid_0's multi_logloss: 0.56803\n",
            "[47]\tvalid_0's multi_logloss: 0.5649\n",
            "[48]\tvalid_0's multi_logloss: 0.563301\n",
            "[49]\tvalid_0's multi_logloss: 0.560532\n",
            "[50]\tvalid_0's multi_logloss: 0.557244\n",
            "[51]\tvalid_0's multi_logloss: 0.555411\n",
            "[52]\tvalid_0's multi_logloss: 0.553229\n",
            "[53]\tvalid_0's multi_logloss: 0.55058\n",
            "[54]\tvalid_0's multi_logloss: 0.54802\n",
            "[55]\tvalid_0's multi_logloss: 0.544679\n",
            "[56]\tvalid_0's multi_logloss: 0.543425\n",
            "[57]\tvalid_0's multi_logloss: 0.540171\n",
            "[58]\tvalid_0's multi_logloss: 0.538417\n",
            "[59]\tvalid_0's multi_logloss: 0.536671\n",
            "[60]\tvalid_0's multi_logloss: 0.536421\n",
            "[61]\tvalid_0's multi_logloss: 0.535197\n",
            "[62]\tvalid_0's multi_logloss: 0.533457\n",
            "[63]\tvalid_0's multi_logloss: 0.532339\n",
            "[64]\tvalid_0's multi_logloss: 0.531404\n",
            "[65]\tvalid_0's multi_logloss: 0.529847\n",
            "[66]\tvalid_0's multi_logloss: 0.528569\n",
            "[67]\tvalid_0's multi_logloss: 0.528301\n",
            "[68]\tvalid_0's multi_logloss: 0.527017\n",
            "[69]\tvalid_0's multi_logloss: 0.525968\n",
            "[70]\tvalid_0's multi_logloss: 0.524079\n",
            "[71]\tvalid_0's multi_logloss: 0.522646\n",
            "[72]\tvalid_0's multi_logloss: 0.521299\n",
            "[73]\tvalid_0's multi_logloss: 0.519915\n",
            "[74]\tvalid_0's multi_logloss: 0.518931\n",
            "[75]\tvalid_0's multi_logloss: 0.517328\n",
            "[76]\tvalid_0's multi_logloss: 0.517218\n",
            "[77]\tvalid_0's multi_logloss: 0.516222\n",
            "[78]\tvalid_0's multi_logloss: 0.515183\n",
            "[79]\tvalid_0's multi_logloss: 0.514116\n",
            "[80]\tvalid_0's multi_logloss: 0.513108\n",
            "[81]\tvalid_0's multi_logloss: 0.51185\n",
            "[82]\tvalid_0's multi_logloss: 0.511167\n",
            "[83]\tvalid_0's multi_logloss: 0.510622\n",
            "[84]\tvalid_0's multi_logloss: 0.509889\n",
            "[85]\tvalid_0's multi_logloss: 0.508397\n",
            "[86]\tvalid_0's multi_logloss: 0.507348\n",
            "[87]\tvalid_0's multi_logloss: 0.506505\n",
            "[88]\tvalid_0's multi_logloss: 0.505034\n",
            "[89]\tvalid_0's multi_logloss: 0.504363\n",
            "[90]\tvalid_0's multi_logloss: 0.503744\n",
            "[91]\tvalid_0's multi_logloss: 0.50316\n",
            "[92]\tvalid_0's multi_logloss: 0.502061\n",
            "[93]\tvalid_0's multi_logloss: 0.501427\n",
            "[94]\tvalid_0's multi_logloss: 0.500322\n",
            "[95]\tvalid_0's multi_logloss: 0.498607\n",
            "[96]\tvalid_0's multi_logloss: 0.497806\n",
            "[97]\tvalid_0's multi_logloss: 0.496863\n",
            "[98]\tvalid_0's multi_logloss: 0.495582\n",
            "[99]\tvalid_0's multi_logloss: 0.49425\n",
            "[100]\tvalid_0's multi_logloss: 0.493795\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LGBMClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "### Light GBM ###\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "LGBM = LGBMClassifier()\n",
        "evals = [(v_input, v_output)]\n",
        "LGBM.fit(t_input, t_output, early_stopping_rounds=100, eval_metric='logloss', eval_set=evals, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  validation 성능\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# validation 데이터 예측\n",
        "val_pred = LGBM.predict(val_input)\n",
        "\n",
        "# 정확도 계산\n",
        "accuracy = accuracy_score(val_output, val_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# 정밀도 계산\n",
        "precision = precision_score(val_output, val_pred, average='macro')\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# 재현율 계산\n",
        "recall = recall_score(val_output, val_pred, average='macro')\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# F1 스코어 계산\n",
        "f1 = f1_score(val_output, val_pred, average='macro')\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUnGXqqSjJfW",
        "outputId": "0dee2a63-c5bf-44d6-b6d4-57700e8131fa"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7652221612726275\n",
            "Precision: 0.7395748360728578\n",
            "Recall: 0.7510934047165931\n",
            "F1 Score: 0.7451593756932641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtNboTT_8fRl"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRGQe_R28iqX",
        "outputId": "954ded4c-d994-4c45-ee64-54a1aadacd2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17:15:23] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"early_stoppings\" } are not used.\n",
            "\n",
            "[0]\ttrain-mlogloss:1.07356\teval-mlogloss:1.08230\n",
            "[1]\ttrain-mlogloss:1.05176\teval-mlogloss:1.06784\n",
            "[2]\ttrain-mlogloss:1.03165\teval-mlogloss:1.05466\n",
            "[3]\ttrain-mlogloss:1.01349\teval-mlogloss:1.04410\n",
            "[4]\ttrain-mlogloss:0.99649\teval-mlogloss:1.03102\n",
            "[5]\ttrain-mlogloss:0.98020\teval-mlogloss:1.02033\n",
            "[6]\ttrain-mlogloss:0.96664\teval-mlogloss:1.01097\n",
            "[7]\ttrain-mlogloss:0.95341\teval-mlogloss:1.00226\n",
            "[8]\ttrain-mlogloss:0.93736\teval-mlogloss:0.99154\n",
            "[9]\ttrain-mlogloss:0.92577\teval-mlogloss:0.98350\n",
            "[10]\ttrain-mlogloss:0.91241\teval-mlogloss:0.97428\n",
            "[11]\ttrain-mlogloss:0.90204\teval-mlogloss:0.96650\n",
            "[12]\ttrain-mlogloss:0.89188\teval-mlogloss:0.95883\n",
            "[13]\ttrain-mlogloss:0.88235\teval-mlogloss:0.95223\n",
            "[14]\ttrain-mlogloss:0.87460\teval-mlogloss:0.94628\n",
            "[15]\ttrain-mlogloss:0.86609\teval-mlogloss:0.93972\n",
            "[16]\ttrain-mlogloss:0.85861\teval-mlogloss:0.93461\n",
            "[17]\ttrain-mlogloss:0.85037\teval-mlogloss:0.92821\n",
            "[18]\ttrain-mlogloss:0.84170\teval-mlogloss:0.92245\n",
            "[19]\ttrain-mlogloss:0.83418\teval-mlogloss:0.91668\n",
            "[20]\ttrain-mlogloss:0.82770\teval-mlogloss:0.91216\n",
            "[21]\ttrain-mlogloss:0.82027\teval-mlogloss:0.90646\n",
            "[22]\ttrain-mlogloss:0.81430\teval-mlogloss:0.90210\n",
            "[23]\ttrain-mlogloss:0.80623\teval-mlogloss:0.89588\n",
            "[24]\ttrain-mlogloss:0.79941\teval-mlogloss:0.89097\n",
            "[25]\ttrain-mlogloss:0.79367\teval-mlogloss:0.88679\n",
            "[26]\ttrain-mlogloss:0.78671\teval-mlogloss:0.88188\n",
            "[27]\ttrain-mlogloss:0.78185\teval-mlogloss:0.87738\n",
            "[28]\ttrain-mlogloss:0.77606\teval-mlogloss:0.87268\n",
            "[29]\ttrain-mlogloss:0.77123\teval-mlogloss:0.86850\n",
            "[30]\ttrain-mlogloss:0.76655\teval-mlogloss:0.86557\n",
            "[31]\ttrain-mlogloss:0.76159\teval-mlogloss:0.86222\n",
            "[32]\ttrain-mlogloss:0.75694\teval-mlogloss:0.85950\n",
            "[33]\ttrain-mlogloss:0.75082\teval-mlogloss:0.85484\n",
            "[34]\ttrain-mlogloss:0.74650\teval-mlogloss:0.85148\n",
            "[35]\ttrain-mlogloss:0.74165\teval-mlogloss:0.84817\n",
            "[36]\ttrain-mlogloss:0.73564\teval-mlogloss:0.84289\n",
            "[37]\ttrain-mlogloss:0.73153\teval-mlogloss:0.83977\n",
            "[38]\ttrain-mlogloss:0.72485\teval-mlogloss:0.83505\n",
            "[39]\ttrain-mlogloss:0.72039\teval-mlogloss:0.83133\n",
            "[40]\ttrain-mlogloss:0.71694\teval-mlogloss:0.82797\n",
            "[41]\ttrain-mlogloss:0.71269\teval-mlogloss:0.82486\n",
            "[42]\ttrain-mlogloss:0.70837\teval-mlogloss:0.82131\n",
            "[43]\ttrain-mlogloss:0.70505\teval-mlogloss:0.81836\n",
            "[44]\ttrain-mlogloss:0.70185\teval-mlogloss:0.81592\n",
            "[45]\ttrain-mlogloss:0.69853\teval-mlogloss:0.81385\n",
            "[46]\ttrain-mlogloss:0.69459\teval-mlogloss:0.81086\n",
            "[47]\ttrain-mlogloss:0.69125\teval-mlogloss:0.80848\n",
            "[48]\ttrain-mlogloss:0.68769\teval-mlogloss:0.80547\n",
            "[49]\ttrain-mlogloss:0.68460\teval-mlogloss:0.80298\n",
            "[50]\ttrain-mlogloss:0.68117\teval-mlogloss:0.80043\n",
            "[51]\ttrain-mlogloss:0.67811\teval-mlogloss:0.79792\n",
            "[52]\ttrain-mlogloss:0.67548\teval-mlogloss:0.79626\n",
            "[53]\ttrain-mlogloss:0.67219\teval-mlogloss:0.79377\n",
            "[54]\ttrain-mlogloss:0.66900\teval-mlogloss:0.79095\n",
            "[55]\ttrain-mlogloss:0.66662\teval-mlogloss:0.78894\n",
            "[56]\ttrain-mlogloss:0.66465\teval-mlogloss:0.78701\n",
            "[57]\ttrain-mlogloss:0.66137\teval-mlogloss:0.78508\n",
            "[58]\ttrain-mlogloss:0.65895\teval-mlogloss:0.78312\n",
            "[59]\ttrain-mlogloss:0.65686\teval-mlogloss:0.78160\n",
            "[60]\ttrain-mlogloss:0.65498\teval-mlogloss:0.78031\n",
            "[61]\ttrain-mlogloss:0.65262\teval-mlogloss:0.77793\n",
            "[62]\ttrain-mlogloss:0.65001\teval-mlogloss:0.77555\n",
            "[63]\ttrain-mlogloss:0.64715\teval-mlogloss:0.77364\n",
            "[64]\ttrain-mlogloss:0.64467\teval-mlogloss:0.77146\n",
            "[65]\ttrain-mlogloss:0.64180\teval-mlogloss:0.76871\n",
            "[66]\ttrain-mlogloss:0.64018\teval-mlogloss:0.76774\n",
            "[67]\ttrain-mlogloss:0.63770\teval-mlogloss:0.76594\n",
            "[68]\ttrain-mlogloss:0.63574\teval-mlogloss:0.76367\n",
            "[69]\ttrain-mlogloss:0.63321\teval-mlogloss:0.76211\n",
            "[70]\ttrain-mlogloss:0.63063\teval-mlogloss:0.75977\n",
            "[71]\ttrain-mlogloss:0.62843\teval-mlogloss:0.75835\n",
            "[72]\ttrain-mlogloss:0.62666\teval-mlogloss:0.75682\n",
            "[73]\ttrain-mlogloss:0.62411\teval-mlogloss:0.75489\n",
            "[74]\ttrain-mlogloss:0.62192\teval-mlogloss:0.75325\n",
            "[75]\ttrain-mlogloss:0.61891\teval-mlogloss:0.75165\n",
            "[76]\ttrain-mlogloss:0.61718\teval-mlogloss:0.75003\n",
            "[77]\ttrain-mlogloss:0.61500\teval-mlogloss:0.74887\n",
            "[78]\ttrain-mlogloss:0.61221\teval-mlogloss:0.74672\n",
            "[79]\ttrain-mlogloss:0.60992\teval-mlogloss:0.74504\n",
            "[80]\ttrain-mlogloss:0.60775\teval-mlogloss:0.74333\n",
            "[81]\ttrain-mlogloss:0.60622\teval-mlogloss:0.74183\n",
            "[82]\ttrain-mlogloss:0.60457\teval-mlogloss:0.74101\n",
            "[83]\ttrain-mlogloss:0.60242\teval-mlogloss:0.73949\n",
            "[84]\ttrain-mlogloss:0.60055\teval-mlogloss:0.73832\n",
            "[85]\ttrain-mlogloss:0.59916\teval-mlogloss:0.73694\n",
            "[86]\ttrain-mlogloss:0.59728\teval-mlogloss:0.73570\n",
            "[87]\ttrain-mlogloss:0.59596\teval-mlogloss:0.73487\n",
            "[88]\ttrain-mlogloss:0.59462\teval-mlogloss:0.73424\n",
            "[89]\ttrain-mlogloss:0.59246\teval-mlogloss:0.73242\n",
            "[90]\ttrain-mlogloss:0.59076\teval-mlogloss:0.73118\n",
            "[91]\ttrain-mlogloss:0.58931\teval-mlogloss:0.73012\n",
            "[92]\ttrain-mlogloss:0.58749\teval-mlogloss:0.72898\n",
            "[93]\ttrain-mlogloss:0.58489\teval-mlogloss:0.72684\n",
            "[94]\ttrain-mlogloss:0.58318\teval-mlogloss:0.72574\n",
            "[95]\ttrain-mlogloss:0.58153\teval-mlogloss:0.72506\n",
            "[96]\ttrain-mlogloss:0.58012\teval-mlogloss:0.72355\n",
            "[97]\ttrain-mlogloss:0.57904\teval-mlogloss:0.72258\n",
            "[98]\ttrain-mlogloss:0.57784\teval-mlogloss:0.72203\n",
            "[99]\ttrain-mlogloss:0.57605\teval-mlogloss:0.72050\n",
            "[100]\ttrain-mlogloss:0.57461\teval-mlogloss:0.71952\n",
            "[101]\ttrain-mlogloss:0.57291\teval-mlogloss:0.71794\n",
            "[102]\ttrain-mlogloss:0.57094\teval-mlogloss:0.71628\n",
            "[103]\ttrain-mlogloss:0.56918\teval-mlogloss:0.71494\n",
            "[104]\ttrain-mlogloss:0.56739\teval-mlogloss:0.71382\n",
            "[105]\ttrain-mlogloss:0.56576\teval-mlogloss:0.71290\n",
            "[106]\ttrain-mlogloss:0.56455\teval-mlogloss:0.71196\n",
            "[107]\ttrain-mlogloss:0.56303\teval-mlogloss:0.71086\n",
            "[108]\ttrain-mlogloss:0.56195\teval-mlogloss:0.71052\n",
            "[109]\ttrain-mlogloss:0.56045\teval-mlogloss:0.70890\n",
            "[110]\ttrain-mlogloss:0.55847\teval-mlogloss:0.70809\n",
            "[111]\ttrain-mlogloss:0.55686\teval-mlogloss:0.70688\n",
            "[112]\ttrain-mlogloss:0.55584\teval-mlogloss:0.70597\n",
            "[113]\ttrain-mlogloss:0.55400\teval-mlogloss:0.70479\n",
            "[114]\ttrain-mlogloss:0.55290\teval-mlogloss:0.70400\n",
            "[115]\ttrain-mlogloss:0.55068\teval-mlogloss:0.70216\n",
            "[116]\ttrain-mlogloss:0.54871\teval-mlogloss:0.70076\n",
            "[117]\ttrain-mlogloss:0.54764\teval-mlogloss:0.70027\n",
            "[118]\ttrain-mlogloss:0.54630\teval-mlogloss:0.69928\n",
            "[119]\ttrain-mlogloss:0.54543\teval-mlogloss:0.69857\n",
            "[120]\ttrain-mlogloss:0.54468\teval-mlogloss:0.69813\n",
            "[121]\ttrain-mlogloss:0.54313\teval-mlogloss:0.69673\n",
            "[122]\ttrain-mlogloss:0.54136\teval-mlogloss:0.69546\n",
            "[123]\ttrain-mlogloss:0.53988\teval-mlogloss:0.69452\n",
            "[124]\ttrain-mlogloss:0.53801\teval-mlogloss:0.69324\n",
            "[125]\ttrain-mlogloss:0.53679\teval-mlogloss:0.69253\n",
            "[126]\ttrain-mlogloss:0.53525\teval-mlogloss:0.69094\n",
            "[127]\ttrain-mlogloss:0.53420\teval-mlogloss:0.69019\n",
            "[128]\ttrain-mlogloss:0.53231\teval-mlogloss:0.68895\n",
            "[129]\ttrain-mlogloss:0.53110\teval-mlogloss:0.68815\n",
            "[130]\ttrain-mlogloss:0.53036\teval-mlogloss:0.68770\n",
            "[131]\ttrain-mlogloss:0.52945\teval-mlogloss:0.68667\n",
            "[132]\ttrain-mlogloss:0.52824\teval-mlogloss:0.68553\n",
            "[133]\ttrain-mlogloss:0.52738\teval-mlogloss:0.68475\n",
            "[134]\ttrain-mlogloss:0.52608\teval-mlogloss:0.68397\n",
            "[135]\ttrain-mlogloss:0.52478\teval-mlogloss:0.68255\n",
            "[136]\ttrain-mlogloss:0.52358\teval-mlogloss:0.68155\n",
            "[137]\ttrain-mlogloss:0.52186\teval-mlogloss:0.68056\n",
            "[138]\ttrain-mlogloss:0.51990\teval-mlogloss:0.67958\n",
            "[139]\ttrain-mlogloss:0.51829\teval-mlogloss:0.67859\n",
            "[140]\ttrain-mlogloss:0.51737\teval-mlogloss:0.67782\n",
            "[141]\ttrain-mlogloss:0.51611\teval-mlogloss:0.67689\n",
            "[142]\ttrain-mlogloss:0.51475\teval-mlogloss:0.67605\n",
            "[143]\ttrain-mlogloss:0.51341\teval-mlogloss:0.67508\n",
            "[144]\ttrain-mlogloss:0.51275\teval-mlogloss:0.67431\n",
            "[145]\ttrain-mlogloss:0.51154\teval-mlogloss:0.67307\n",
            "[146]\ttrain-mlogloss:0.51065\teval-mlogloss:0.67236\n",
            "[147]\ttrain-mlogloss:0.50984\teval-mlogloss:0.67204\n",
            "[148]\ttrain-mlogloss:0.50851\teval-mlogloss:0.67104\n",
            "[149]\ttrain-mlogloss:0.50792\teval-mlogloss:0.67071\n",
            "[150]\ttrain-mlogloss:0.50664\teval-mlogloss:0.66951\n",
            "[151]\ttrain-mlogloss:0.50567\teval-mlogloss:0.66869\n",
            "[152]\ttrain-mlogloss:0.50459\teval-mlogloss:0.66766\n",
            "[153]\ttrain-mlogloss:0.50365\teval-mlogloss:0.66719\n",
            "[154]\ttrain-mlogloss:0.50277\teval-mlogloss:0.66680\n",
            "[155]\ttrain-mlogloss:0.50193\teval-mlogloss:0.66660\n",
            "[156]\ttrain-mlogloss:0.50057\teval-mlogloss:0.66591\n",
            "[157]\ttrain-mlogloss:0.49993\teval-mlogloss:0.66513\n",
            "[158]\ttrain-mlogloss:0.49865\teval-mlogloss:0.66465\n",
            "[159]\ttrain-mlogloss:0.49767\teval-mlogloss:0.66360\n",
            "[160]\ttrain-mlogloss:0.49655\teval-mlogloss:0.66259\n",
            "[161]\ttrain-mlogloss:0.49543\teval-mlogloss:0.66225\n",
            "[162]\ttrain-mlogloss:0.49415\teval-mlogloss:0.66112\n",
            "[163]\ttrain-mlogloss:0.49304\teval-mlogloss:0.65991\n",
            "[164]\ttrain-mlogloss:0.49166\teval-mlogloss:0.65906\n",
            "[165]\ttrain-mlogloss:0.49074\teval-mlogloss:0.65885\n",
            "[166]\ttrain-mlogloss:0.49021\teval-mlogloss:0.65869\n",
            "[167]\ttrain-mlogloss:0.48917\teval-mlogloss:0.65752\n",
            "[168]\ttrain-mlogloss:0.48837\teval-mlogloss:0.65703\n",
            "[169]\ttrain-mlogloss:0.48690\teval-mlogloss:0.65601\n",
            "[170]\ttrain-mlogloss:0.48572\teval-mlogloss:0.65522\n",
            "[171]\ttrain-mlogloss:0.48495\teval-mlogloss:0.65458\n",
            "[172]\ttrain-mlogloss:0.48421\teval-mlogloss:0.65407\n",
            "[173]\ttrain-mlogloss:0.48305\teval-mlogloss:0.65324\n",
            "[174]\ttrain-mlogloss:0.48215\teval-mlogloss:0.65237\n",
            "[175]\ttrain-mlogloss:0.48096\teval-mlogloss:0.65143\n",
            "[176]\ttrain-mlogloss:0.47986\teval-mlogloss:0.65096\n",
            "[177]\ttrain-mlogloss:0.47911\teval-mlogloss:0.65041\n",
            "[178]\ttrain-mlogloss:0.47823\teval-mlogloss:0.64992\n",
            "[179]\ttrain-mlogloss:0.47738\teval-mlogloss:0.64923\n",
            "[180]\ttrain-mlogloss:0.47645\teval-mlogloss:0.64856\n",
            "[181]\ttrain-mlogloss:0.47525\teval-mlogloss:0.64781\n",
            "[182]\ttrain-mlogloss:0.47426\teval-mlogloss:0.64714\n",
            "[183]\ttrain-mlogloss:0.47318\teval-mlogloss:0.64614\n",
            "[184]\ttrain-mlogloss:0.47221\teval-mlogloss:0.64550\n",
            "[185]\ttrain-mlogloss:0.47095\teval-mlogloss:0.64477\n",
            "[186]\ttrain-mlogloss:0.46995\teval-mlogloss:0.64429\n",
            "[187]\ttrain-mlogloss:0.46908\teval-mlogloss:0.64353\n",
            "[188]\ttrain-mlogloss:0.46755\teval-mlogloss:0.64245\n",
            "[189]\ttrain-mlogloss:0.46697\teval-mlogloss:0.64237\n",
            "[190]\ttrain-mlogloss:0.46643\teval-mlogloss:0.64212\n",
            "[191]\ttrain-mlogloss:0.46573\teval-mlogloss:0.64149\n",
            "[192]\ttrain-mlogloss:0.46471\teval-mlogloss:0.64095\n",
            "[193]\ttrain-mlogloss:0.46399\teval-mlogloss:0.64033\n",
            "[194]\ttrain-mlogloss:0.46305\teval-mlogloss:0.63982\n",
            "[195]\ttrain-mlogloss:0.46239\teval-mlogloss:0.63951\n",
            "[196]\ttrain-mlogloss:0.46181\teval-mlogloss:0.63919\n",
            "[197]\ttrain-mlogloss:0.46122\teval-mlogloss:0.63916\n",
            "[198]\ttrain-mlogloss:0.46031\teval-mlogloss:0.63840\n",
            "[199]\ttrain-mlogloss:0.45940\teval-mlogloss:0.63782\n",
            "[200]\ttrain-mlogloss:0.45836\teval-mlogloss:0.63743\n",
            "[201]\ttrain-mlogloss:0.45785\teval-mlogloss:0.63711\n",
            "[202]\ttrain-mlogloss:0.45686\teval-mlogloss:0.63645\n",
            "[203]\ttrain-mlogloss:0.45600\teval-mlogloss:0.63574\n",
            "[204]\ttrain-mlogloss:0.45502\teval-mlogloss:0.63496\n",
            "[205]\ttrain-mlogloss:0.45437\teval-mlogloss:0.63460\n",
            "[206]\ttrain-mlogloss:0.45364\teval-mlogloss:0.63405\n",
            "[207]\ttrain-mlogloss:0.45247\teval-mlogloss:0.63314\n",
            "[208]\ttrain-mlogloss:0.45162\teval-mlogloss:0.63245\n",
            "[209]\ttrain-mlogloss:0.45089\teval-mlogloss:0.63226\n",
            "[210]\ttrain-mlogloss:0.45047\teval-mlogloss:0.63199\n",
            "[211]\ttrain-mlogloss:0.44957\teval-mlogloss:0.63141\n",
            "[212]\ttrain-mlogloss:0.44872\teval-mlogloss:0.63081\n",
            "[213]\ttrain-mlogloss:0.44784\teval-mlogloss:0.63031\n",
            "[214]\ttrain-mlogloss:0.44698\teval-mlogloss:0.62979\n",
            "[215]\ttrain-mlogloss:0.44632\teval-mlogloss:0.62929\n",
            "[216]\ttrain-mlogloss:0.44547\teval-mlogloss:0.62897\n",
            "[217]\ttrain-mlogloss:0.44456\teval-mlogloss:0.62838\n",
            "[218]\ttrain-mlogloss:0.44371\teval-mlogloss:0.62797\n",
            "[219]\ttrain-mlogloss:0.44322\teval-mlogloss:0.62773\n",
            "[220]\ttrain-mlogloss:0.44264\teval-mlogloss:0.62757\n",
            "[221]\ttrain-mlogloss:0.44146\teval-mlogloss:0.62705\n",
            "[222]\ttrain-mlogloss:0.44069\teval-mlogloss:0.62676\n",
            "[223]\ttrain-mlogloss:0.43946\teval-mlogloss:0.62567\n",
            "[224]\ttrain-mlogloss:0.43862\teval-mlogloss:0.62513\n",
            "[225]\ttrain-mlogloss:0.43768\teval-mlogloss:0.62430\n",
            "[226]\ttrain-mlogloss:0.43682\teval-mlogloss:0.62390\n",
            "[227]\ttrain-mlogloss:0.43611\teval-mlogloss:0.62321\n",
            "[228]\ttrain-mlogloss:0.43516\teval-mlogloss:0.62263\n",
            "[229]\ttrain-mlogloss:0.43439\teval-mlogloss:0.62223\n",
            "[230]\ttrain-mlogloss:0.43370\teval-mlogloss:0.62186\n",
            "[231]\ttrain-mlogloss:0.43302\teval-mlogloss:0.62147\n",
            "[232]\ttrain-mlogloss:0.43159\teval-mlogloss:0.62049\n",
            "[233]\ttrain-mlogloss:0.43056\teval-mlogloss:0.61981\n",
            "[234]\ttrain-mlogloss:0.42988\teval-mlogloss:0.61938\n",
            "[235]\ttrain-mlogloss:0.42898\teval-mlogloss:0.61859\n",
            "[236]\ttrain-mlogloss:0.42844\teval-mlogloss:0.61836\n",
            "[237]\ttrain-mlogloss:0.42796\teval-mlogloss:0.61814\n",
            "[238]\ttrain-mlogloss:0.42735\teval-mlogloss:0.61786\n",
            "[239]\ttrain-mlogloss:0.42633\teval-mlogloss:0.61733\n",
            "[240]\ttrain-mlogloss:0.42585\teval-mlogloss:0.61715\n",
            "[241]\ttrain-mlogloss:0.42528\teval-mlogloss:0.61694\n",
            "[242]\ttrain-mlogloss:0.42472\teval-mlogloss:0.61666\n",
            "[243]\ttrain-mlogloss:0.42409\teval-mlogloss:0.61629\n",
            "[244]\ttrain-mlogloss:0.42350\teval-mlogloss:0.61601\n",
            "[245]\ttrain-mlogloss:0.42299\teval-mlogloss:0.61556\n",
            "[246]\ttrain-mlogloss:0.42219\teval-mlogloss:0.61514\n",
            "[247]\ttrain-mlogloss:0.42166\teval-mlogloss:0.61467\n",
            "[248]\ttrain-mlogloss:0.42083\teval-mlogloss:0.61415\n",
            "[249]\ttrain-mlogloss:0.42017\teval-mlogloss:0.61355\n",
            "[250]\ttrain-mlogloss:0.41931\teval-mlogloss:0.61320\n",
            "[251]\ttrain-mlogloss:0.41869\teval-mlogloss:0.61323\n",
            "[252]\ttrain-mlogloss:0.41781\teval-mlogloss:0.61263\n",
            "[253]\ttrain-mlogloss:0.41733\teval-mlogloss:0.61240\n",
            "[254]\ttrain-mlogloss:0.41669\teval-mlogloss:0.61178\n",
            "[255]\ttrain-mlogloss:0.41600\teval-mlogloss:0.61134\n",
            "[256]\ttrain-mlogloss:0.41494\teval-mlogloss:0.61099\n",
            "[257]\ttrain-mlogloss:0.41417\teval-mlogloss:0.61062\n",
            "[258]\ttrain-mlogloss:0.41371\teval-mlogloss:0.61035\n",
            "[259]\ttrain-mlogloss:0.41289\teval-mlogloss:0.60981\n",
            "[260]\ttrain-mlogloss:0.41212\teval-mlogloss:0.60966\n",
            "[261]\ttrain-mlogloss:0.41155\teval-mlogloss:0.60925\n",
            "[262]\ttrain-mlogloss:0.41100\teval-mlogloss:0.60890\n",
            "[263]\ttrain-mlogloss:0.41024\teval-mlogloss:0.60836\n",
            "[264]\ttrain-mlogloss:0.40967\teval-mlogloss:0.60818\n",
            "[265]\ttrain-mlogloss:0.40910\teval-mlogloss:0.60804\n",
            "[266]\ttrain-mlogloss:0.40832\teval-mlogloss:0.60769\n",
            "[267]\ttrain-mlogloss:0.40745\teval-mlogloss:0.60717\n",
            "[268]\ttrain-mlogloss:0.40694\teval-mlogloss:0.60699\n",
            "[269]\ttrain-mlogloss:0.40625\teval-mlogloss:0.60661\n",
            "[270]\ttrain-mlogloss:0.40552\teval-mlogloss:0.60631\n",
            "[271]\ttrain-mlogloss:0.40506\teval-mlogloss:0.60602\n",
            "[272]\ttrain-mlogloss:0.40433\teval-mlogloss:0.60569\n",
            "[273]\ttrain-mlogloss:0.40364\teval-mlogloss:0.60539\n",
            "[274]\ttrain-mlogloss:0.40316\teval-mlogloss:0.60526\n",
            "[275]\ttrain-mlogloss:0.40233\teval-mlogloss:0.60475\n",
            "[276]\ttrain-mlogloss:0.40154\teval-mlogloss:0.60446\n",
            "[277]\ttrain-mlogloss:0.40077\teval-mlogloss:0.60400\n",
            "[278]\ttrain-mlogloss:0.40021\teval-mlogloss:0.60379\n",
            "[279]\ttrain-mlogloss:0.39994\teval-mlogloss:0.60374\n",
            "[280]\ttrain-mlogloss:0.39955\teval-mlogloss:0.60326\n",
            "[281]\ttrain-mlogloss:0.39893\teval-mlogloss:0.60302\n",
            "[282]\ttrain-mlogloss:0.39840\teval-mlogloss:0.60270\n",
            "[283]\ttrain-mlogloss:0.39792\teval-mlogloss:0.60231\n",
            "[284]\ttrain-mlogloss:0.39722\teval-mlogloss:0.60192\n",
            "[285]\ttrain-mlogloss:0.39684\teval-mlogloss:0.60168\n",
            "[286]\ttrain-mlogloss:0.39611\teval-mlogloss:0.60119\n",
            "[287]\ttrain-mlogloss:0.39550\teval-mlogloss:0.60085\n",
            "[288]\ttrain-mlogloss:0.39498\teval-mlogloss:0.60057\n",
            "[289]\ttrain-mlogloss:0.39413\teval-mlogloss:0.60000\n",
            "[290]\ttrain-mlogloss:0.39360\teval-mlogloss:0.59966\n",
            "[291]\ttrain-mlogloss:0.39257\teval-mlogloss:0.59925\n",
            "[292]\ttrain-mlogloss:0.39194\teval-mlogloss:0.59884\n",
            "[293]\ttrain-mlogloss:0.39128\teval-mlogloss:0.59842\n",
            "[294]\ttrain-mlogloss:0.39049\teval-mlogloss:0.59819\n",
            "[295]\ttrain-mlogloss:0.38991\teval-mlogloss:0.59804\n",
            "[296]\ttrain-mlogloss:0.38931\teval-mlogloss:0.59778\n",
            "[297]\ttrain-mlogloss:0.38865\teval-mlogloss:0.59745\n",
            "[298]\ttrain-mlogloss:0.38817\teval-mlogloss:0.59733\n",
            "[299]\ttrain-mlogloss:0.38747\teval-mlogloss:0.59694\n",
            "[300]\ttrain-mlogloss:0.38696\teval-mlogloss:0.59674\n",
            "[301]\ttrain-mlogloss:0.38611\teval-mlogloss:0.59619\n",
            "[302]\ttrain-mlogloss:0.38551\teval-mlogloss:0.59606\n",
            "[303]\ttrain-mlogloss:0.38482\teval-mlogloss:0.59587\n",
            "[304]\ttrain-mlogloss:0.38434\teval-mlogloss:0.59547\n",
            "[305]\ttrain-mlogloss:0.38346\teval-mlogloss:0.59492\n",
            "[306]\ttrain-mlogloss:0.38284\teval-mlogloss:0.59470\n",
            "[307]\ttrain-mlogloss:0.38251\teval-mlogloss:0.59489\n",
            "[308]\ttrain-mlogloss:0.38194\teval-mlogloss:0.59455\n",
            "[309]\ttrain-mlogloss:0.38125\teval-mlogloss:0.59396\n",
            "[310]\ttrain-mlogloss:0.38042\teval-mlogloss:0.59355\n",
            "[311]\ttrain-mlogloss:0.37990\teval-mlogloss:0.59334\n",
            "[312]\ttrain-mlogloss:0.37940\teval-mlogloss:0.59326\n",
            "[313]\ttrain-mlogloss:0.37881\teval-mlogloss:0.59309\n",
            "[314]\ttrain-mlogloss:0.37794\teval-mlogloss:0.59251\n",
            "[315]\ttrain-mlogloss:0.37720\teval-mlogloss:0.59198\n",
            "[316]\ttrain-mlogloss:0.37661\teval-mlogloss:0.59168\n",
            "[317]\ttrain-mlogloss:0.37621\teval-mlogloss:0.59149\n",
            "[318]\ttrain-mlogloss:0.37564\teval-mlogloss:0.59133\n",
            "[319]\ttrain-mlogloss:0.37524\teval-mlogloss:0.59136\n",
            "[320]\ttrain-mlogloss:0.37470\teval-mlogloss:0.59108\n",
            "[321]\ttrain-mlogloss:0.37430\teval-mlogloss:0.59091\n",
            "[322]\ttrain-mlogloss:0.37380\teval-mlogloss:0.59076\n",
            "[323]\ttrain-mlogloss:0.37318\teval-mlogloss:0.59044\n",
            "[324]\ttrain-mlogloss:0.37248\teval-mlogloss:0.58990\n",
            "[325]\ttrain-mlogloss:0.37215\teval-mlogloss:0.58950\n",
            "[326]\ttrain-mlogloss:0.37183\teval-mlogloss:0.58921\n",
            "[327]\ttrain-mlogloss:0.37121\teval-mlogloss:0.58906\n",
            "[328]\ttrain-mlogloss:0.37056\teval-mlogloss:0.58877\n",
            "[329]\ttrain-mlogloss:0.37016\teval-mlogloss:0.58840\n",
            "[330]\ttrain-mlogloss:0.36992\teval-mlogloss:0.58812\n",
            "[331]\ttrain-mlogloss:0.36945\teval-mlogloss:0.58799\n",
            "[332]\ttrain-mlogloss:0.36918\teval-mlogloss:0.58809\n",
            "[333]\ttrain-mlogloss:0.36838\teval-mlogloss:0.58756\n",
            "[334]\ttrain-mlogloss:0.36784\teval-mlogloss:0.58747\n",
            "[335]\ttrain-mlogloss:0.36762\teval-mlogloss:0.58732\n",
            "[336]\ttrain-mlogloss:0.36676\teval-mlogloss:0.58691\n",
            "[337]\ttrain-mlogloss:0.36622\teval-mlogloss:0.58682\n",
            "[338]\ttrain-mlogloss:0.36590\teval-mlogloss:0.58663\n",
            "[339]\ttrain-mlogloss:0.36540\teval-mlogloss:0.58630\n",
            "[340]\ttrain-mlogloss:0.36459\teval-mlogloss:0.58594\n",
            "[341]\ttrain-mlogloss:0.36397\teval-mlogloss:0.58559\n",
            "[342]\ttrain-mlogloss:0.36360\teval-mlogloss:0.58525\n",
            "[343]\ttrain-mlogloss:0.36306\teval-mlogloss:0.58487\n",
            "[344]\ttrain-mlogloss:0.36239\teval-mlogloss:0.58456\n",
            "[345]\ttrain-mlogloss:0.36210\teval-mlogloss:0.58463\n",
            "[346]\ttrain-mlogloss:0.36186\teval-mlogloss:0.58479\n",
            "[347]\ttrain-mlogloss:0.36134\teval-mlogloss:0.58467\n",
            "[348]\ttrain-mlogloss:0.36081\teval-mlogloss:0.58437\n",
            "[349]\ttrain-mlogloss:0.36012\teval-mlogloss:0.58377\n",
            "[350]\ttrain-mlogloss:0.35959\teval-mlogloss:0.58331\n",
            "[351]\ttrain-mlogloss:0.35926\teval-mlogloss:0.58298\n",
            "[352]\ttrain-mlogloss:0.35906\teval-mlogloss:0.58285\n",
            "[353]\ttrain-mlogloss:0.35851\teval-mlogloss:0.58238\n",
            "[354]\ttrain-mlogloss:0.35789\teval-mlogloss:0.58183\n",
            "[355]\ttrain-mlogloss:0.35738\teval-mlogloss:0.58175\n",
            "[356]\ttrain-mlogloss:0.35683\teval-mlogloss:0.58110\n",
            "[357]\ttrain-mlogloss:0.35655\teval-mlogloss:0.58075\n",
            "[358]\ttrain-mlogloss:0.35642\teval-mlogloss:0.58088\n",
            "[359]\ttrain-mlogloss:0.35580\teval-mlogloss:0.58059\n",
            "[360]\ttrain-mlogloss:0.35553\teval-mlogloss:0.58051\n",
            "[361]\ttrain-mlogloss:0.35489\teval-mlogloss:0.57989\n",
            "[362]\ttrain-mlogloss:0.35467\teval-mlogloss:0.57977\n",
            "[363]\ttrain-mlogloss:0.35448\teval-mlogloss:0.57962\n",
            "[364]\ttrain-mlogloss:0.35421\teval-mlogloss:0.57952\n",
            "[365]\ttrain-mlogloss:0.35343\teval-mlogloss:0.57920\n",
            "[366]\ttrain-mlogloss:0.35309\teval-mlogloss:0.57902\n",
            "[367]\ttrain-mlogloss:0.35263\teval-mlogloss:0.57859\n",
            "[368]\ttrain-mlogloss:0.35217\teval-mlogloss:0.57827\n",
            "[369]\ttrain-mlogloss:0.35200\teval-mlogloss:0.57823\n",
            "[370]\ttrain-mlogloss:0.35165\teval-mlogloss:0.57795\n",
            "[371]\ttrain-mlogloss:0.35126\teval-mlogloss:0.57786\n",
            "[372]\ttrain-mlogloss:0.35068\teval-mlogloss:0.57745\n",
            "[373]\ttrain-mlogloss:0.35044\teval-mlogloss:0.57738\n",
            "[374]\ttrain-mlogloss:0.34990\teval-mlogloss:0.57698\n",
            "[375]\ttrain-mlogloss:0.34907\teval-mlogloss:0.57623\n",
            "[376]\ttrain-mlogloss:0.34822\teval-mlogloss:0.57568\n",
            "[377]\ttrain-mlogloss:0.34757\teval-mlogloss:0.57522\n",
            "[378]\ttrain-mlogloss:0.34717\teval-mlogloss:0.57510\n",
            "[379]\ttrain-mlogloss:0.34674\teval-mlogloss:0.57494\n",
            "[380]\ttrain-mlogloss:0.34607\teval-mlogloss:0.57439\n",
            "[381]\ttrain-mlogloss:0.34558\teval-mlogloss:0.57419\n",
            "[382]\ttrain-mlogloss:0.34518\teval-mlogloss:0.57408\n",
            "[383]\ttrain-mlogloss:0.34471\teval-mlogloss:0.57388\n",
            "[384]\ttrain-mlogloss:0.34379\teval-mlogloss:0.57337\n",
            "[385]\ttrain-mlogloss:0.34325\teval-mlogloss:0.57303\n",
            "[386]\ttrain-mlogloss:0.34279\teval-mlogloss:0.57285\n",
            "[387]\ttrain-mlogloss:0.34234\teval-mlogloss:0.57260\n",
            "[388]\ttrain-mlogloss:0.34175\teval-mlogloss:0.57222\n",
            "[389]\ttrain-mlogloss:0.34123\teval-mlogloss:0.57177\n",
            "[390]\ttrain-mlogloss:0.34074\teval-mlogloss:0.57161\n",
            "[391]\ttrain-mlogloss:0.34041\teval-mlogloss:0.57142\n",
            "[392]\ttrain-mlogloss:0.33993\teval-mlogloss:0.57096\n",
            "[393]\ttrain-mlogloss:0.33974\teval-mlogloss:0.57084\n",
            "[394]\ttrain-mlogloss:0.33918\teval-mlogloss:0.57052\n",
            "[395]\ttrain-mlogloss:0.33877\teval-mlogloss:0.57034\n",
            "[396]\ttrain-mlogloss:0.33828\teval-mlogloss:0.57023\n",
            "[397]\ttrain-mlogloss:0.33807\teval-mlogloss:0.56998\n",
            "[398]\ttrain-mlogloss:0.33756\teval-mlogloss:0.56974\n",
            "[399]\ttrain-mlogloss:0.33709\teval-mlogloss:0.56970\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "params = {'max_depth' : 3,\n",
        "         'eta' : 0.1, \n",
        "         'objective' : 'multi:softmax',\n",
        "         'num_class' : 3,\n",
        "         'eval_metric' : 'mlogloss',\n",
        "         'early_stoppings' : 100 }\n",
        "\n",
        "num_rounds = 400\n",
        "\n",
        "dt = xgb.DMatrix(data=t_input, label=t_output)\n",
        "dv = xgb.DMatrix(data=v_input, label=v_output)\n",
        "\n",
        "wlist=[(dt, 'train'), (dv, 'eval')]\n",
        "xgb_model = xgb.train(params=params, dtrain=dt, num_boost_round=num_rounds, evals=wlist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uce4hUVbJ8Ae",
        "outputId": "b09c11fb-955e-4379-cb78-c821f9e6d45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7185957213384531\n",
            "Precision: 0.676744492771816\n",
            "Recall: 0.7284604016488073\n",
            "F1 Score: 0.6988499773771236\n"
          ]
        }
      ],
      "source": [
        "#  validation 성능\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# validation 데이터 예측\n",
        "\n",
        "dval = xgb.DMatrix(data=val_input, label=val_output)\n",
        "\n",
        "val_pred = xgb_model.predict(dval)\n",
        "\n",
        "# 정확도 계산\n",
        "accuracy = accuracy_score(val_output, val_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# 정밀도 계산\n",
        "precision = precision_score(val_output, val_pred, average='macro')\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# 재현율 계산\n",
        "recall = recall_score(val_output, val_pred, average='macro')\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# F1 스코어 계산\n",
        "f1 = f1_score(val_output, val_pred, average='macro')\n",
        "print(\"F1 Score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 결론\n",
        "\n",
        "data leaking을 방지하기 위해 validation 데이터셋을 직접 지정할 수 있는 모델 두 가지를 비교해 보았다.\n",
        "\n",
        "이 중 recall이 더 높은 LightGBM을 모델을 사용하자."
      ],
      "metadata": {
        "id": "-lnx65TsqlUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test 데이터 예측\n",
        "\n",
        "test_input =  scaler_fit.transform(test_input_data)\n",
        "\n",
        "test_output = test_output_data\n",
        "\n",
        "test_pred = LGBM.predict(test_input)\n",
        "\n",
        "# 정확도 계산\n",
        "accuracy = accuracy_score(test_output, test_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# 정밀도 계산\n",
        "precision = precision_score(test_output, test_pred, average='macro')\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# 재현율 계산\n",
        "recall = recall_score(test_output, test_pred, average='macro')\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# F1 스코어 계산\n",
        "f1 = f1_score(test_output, test_pred, average='macro')\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hox3ZxFF1wZ7",
        "outputId": "f3110d84-0257-4e2a-f6db-7554756178d1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7726075504828798\n",
            "Precision: 0.7308847434425226\n",
            "Recall: 0.7808662772477639\n",
            "F1 Score: 0.7526889999748554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "notJoc8eogQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddcb0f1b-d39e-4545-f0d2-970fee16eccb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/빅데이터 팀플/model/final_lgbm_scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(scaler_fit, '/content/drive/MyDrive/빅데이터 팀플/model/final_lgbm_scaler.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SZJs61nfImL",
        "outputId": "fa3ebb0d-a2d2-4efd-d687-46cb79780852"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/빅데이터 팀플/model/final_lgbm_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(LGBM, '/content/drive/MyDrive/빅데이터 팀플/model/final_lgbm_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y4gUcQTh2fNj"
      },
      "execution_count": 40,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}